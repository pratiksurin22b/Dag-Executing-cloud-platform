<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/scheduler-service/src/main/java/com/example/demo/service/OrchestratorService.java">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/scheduler-service/src/main/java/com/example/demo/service/OrchestratorService.java" />
              <option name="originalContent" value="package com.example.demo.service;&#10;&#10;// All existing imports from your v18 file...&#10;import com.example.demo.config.RabbitMQConfig;&#10;import com.example.demo.dto.DagStatusResponse;&#10;import com.example.demo.dto.SystemStatusResponse; // NEW Import&#10;import com.fasterxml.jackson.core.type.TypeReference;&#10;import com.fasterxml.jackson.databind.JsonNode;&#10;import com.fasterxml.jackson.databind.ObjectMapper;&#10;import org.slf4j.Logger;&#10;import org.slf4j.LoggerFactory;&#10;import org.springframework.beans.factory.annotation.Autowired;&#10;import org.springframework.data.redis.core.StringRedisTemplate;&#10;import org.springframework.stereotype.Service;&#10;&#10;import java.io.IOException;&#10;import java.io.InputStream;&#10;import java.util.ArrayList;&#10;import java.util.Arrays; // NEW Import&#10;import java.util.Collections;&#10;import java.util.List;&#10;import java.util.Map; // NEW Import&#10;import java.util.UUID;&#10;import java.util.stream.Collectors; // NEW Import&#10;import java.util.stream.StreamSupport;&#10;&#10;// Kubernetes Imports (v17.x)&#10;import io.kubernetes.client.openapi.ApiClient;&#10;import io.kubernetes.client.openapi.ApiException;&#10;import io.kubernetes.client.openapi.Configuration;&#10;import io.kubernetes.client.openapi.apis.BatchV1Api;&#10;import io.kubernetes.client.openapi.apis.CoreV1Api;&#10;import io.kubernetes.client.openapi.models.*; // Keep model imports&#10;import io.kubernetes.client.util.ClientBuilder;&#10;import io.kubernetes.client.util.Streams;&#10;import io.kubernetes.client.informer.SharedInformerFactory;&#10;import io.kubernetes.client.informer.cache.Indexer;&#10;import io.kubernetes.client.informer.ResourceEventHandler;&#10;import io.kubernetes.client.util.CallGeneratorParams;&#10;import okhttp3.Call;&#10;// import io.kubernetes.client.util.generic.GenericKubernetesApi; // This was from v19+, v18 uses direct call&#10;&#10;import jakarta.annotation.PostConstruct;&#10;import jakarta.annotation.PreDestroy;&#10;&#10;/**&#10; * The OrchestratorService is the &quot;brain&quot; managing DAG execution via Kubernetes Jobs (v18.0.0 client).&#10; */&#10;@Service&#10;public class OrchestratorService {&#10;&#10;    private static final Logger LOGGER = LoggerFactory.getLogger(OrchestratorService.class);&#10;    private final StringRedisTemplate redisTemplate;&#10;    private final ObjectMapper objectMapper;&#10;    private final BatchV1Api batchV1Api;&#10;    private final CoreV1Api coreV1Api;&#10;    private final SharedInformerFactory informerFactory;&#10;    private Indexer&lt;V1Job&gt; jobIndexer;&#10;&#10;    // Map of our core components and their app labels in Kubernetes&#10;    private static final Map&lt;String, String&gt; CORE_SERVICES = Map.of(&#10;            &quot;Scheduler&quot;, &quot;scheduler-service&quot;,&#10;            &quot;Frontend&quot;, &quot;frontend&quot;,&#10;            &quot;Redis&quot;, &quot;redis-ha&quot;, // Using the HA name from Phase 8.1&#10;            &quot;RabbitMQ&quot;, &quot;rabbitmq&quot; // Assuming app label 'rabbitmq' from Phase 8.2&#10;    );&#10;&#10;&#10;    @Autowired&#10;    public OrchestratorService(StringRedisTemplate redisTemplate, ObjectMapper objectMapper) throws IOException {&#10;        this.redisTemplate = redisTemplate;&#10;        this.objectMapper = objectMapper;&#10;&#10;        ApiClient client = ClientBuilder.cluster().build();&#10;        client.setReadTimeout(0);&#10;        client.setWriteTimeout(0);&#10;        client.setConnectTimeout(0);&#10;        Configuration.setDefaultApiClient(client);&#10;        this.batchV1Api = new BatchV1Api(client);&#10;        this.coreV1Api = new CoreV1Api(client);&#10;        this.informerFactory = new SharedInformerFactory(client);&#10;&#10;        setupJobWatcher();&#10;&#10;        LOGGER.info(&quot;Kubernetes client and Job watcher initialized successfully.&quot;);&#10;    }&#10;&#10;    // --- Configure the Kubernetes Job Watcher (Corrected for v18.0.0 API Call Signature) ---&#10;    private void setupJobWatcher() {&#10;        var jobInformer = informerFactory.sharedIndexInformerFor(&#10;                (CallGeneratorParams params) -&gt; {&#10;                    try {&#10;                        return batchV1Api.listNamespacedJobCall(&#10;                                &quot;helios&quot;, // namespace&#10;                                null, // pretty&#10;                                null, // allowWatchBookmarks&#10;                                null, // _continue&#10;                                null, // fieldSelector&#10;                                &quot;app=helios-task&quot;, // labelSelector to only watch our jobs&#10;                                null, // limit&#10;                                params.resourceVersion, // resourceVersion from params&#10;                                null, // resourceVersionMatch&#10;                                params.timeoutSeconds, // timeoutSeconds from params&#10;                                params.watch, // watch from params&#10;                                null // _callback&#10;                        );&#10;                    } catch (ApiException e) {&#10;                        LOGGER.error(&quot;Watcher: Failed to create list call for Jobs: {}&quot;, e.getResponseBody(), e);&#10;                        throw new RuntimeException(e);&#10;                    }&#10;                },&#10;                V1Job.class, V1JobList.class&#10;        );&#10;&#10;        // --- Event Handler logic (Unchanged) ---&#10;        jobInformer.addEventHandler(new ResourceEventHandler&lt;V1Job&gt;() {&#10;            @Override public void onAdd(V1Job job) { /* Ignore */ }&#10;            @Override&#10;            public void onUpdate(V1Job oldJob, V1Job newJob) {&#10;                // ... (existing watcher logic as before) ...&#10;                String jobName = newJob.getMetadata().getName();&#10;                LOGGER.debug(&quot;Watcher: Job updated - {}&quot;, jobName);&#10;                var status = newJob.getStatus();&#10;                if (status != null) {&#10;                    boolean succeeded = status.getSucceeded() != null &amp;&amp; status.getSucceeded() &gt; 0;&#10;                    boolean failed = status.getFailed() != null &amp;&amp; status.getFailed() &gt; 0;&#10;                    if (succeeded || failed) {&#10;                        String processedKey = &quot;job:processed:&quot; + jobName;&#10;                        if (Boolean.TRUE.equals(redisTemplate.opsForValue().setIfAbsent(processedKey, &quot;true&quot;))) {&#10;                            redisTemplate.expire(processedKey, java.time.Duration.ofHours(2));&#10;                            LOGGER.info(&quot;Watcher: Job '{}' completed (Succeeded={}, Failed={}). Processing result...&quot;, jobName, succeeded, failed);&#10;                            handleJobCompletion(newJob, succeeded);&#10;                        } else {&#10;                            LOGGER.debug(&quot;Watcher: Job '{}' completion already processed.&quot;, jobName);&#10;                        }&#10;                    }&#10;                }&#10;            }&#10;            @Override public void onDelete(V1Job job, boolean deletedFinalStateUnknown) { /* Ignore */ }&#10;        });&#10;        this.jobIndexer = jobInformer.getIndexer();&#10;    }&#10;&#10;    // --- Start/Stop watchers (Unchanged) ---&#10;    @PostConstruct&#10;    public void startWatchers() {&#10;        LOGGER.info(&quot;Starting Kubernetes watchers...&quot;);&#10;        informerFactory.startAllRegisteredInformers();&#10;        LOGGER.info(&quot;Kubernetes watchers started.&quot;);&#10;    }&#10;    @PreDestroy&#10;    public void stopWatchers() {&#10;        LOGGER.info(&quot;Stopping Kubernetes watchers...&quot;);&#10;        informerFactory.stopAllRegisteredInformers(true); // Graceful shutdown&#10;        LOGGER.info(&quot;Kubernetes watchers stopped.&quot;);&#10;    }&#10;&#10;    // --- NEW METHOD: Get System Health Status ---&#10;    public SystemStatusResponse getSystemStatus() throws ApiException {&#10;        List&lt;SystemStatusResponse.ServiceStatus&gt; serviceStatuses = new ArrayList&lt;&gt;();&#10;        String namespace = &quot;helios&quot;;&#10;&#10;        V1PodList allPods = coreV1Api.listNamespacedPod(&#10;                namespace, null, null, null, null, null, null, null, null, null, null&#10;        );&#10;        Map&lt;String, List&lt;V1Pod&gt;&gt; podsByApp = allPods.getItems().stream()&#10;                .filter(pod -&gt; pod.getMetadata() != null &amp;&amp; pod.getMetadata().getLabels() != null)&#10;                .collect(Collectors.groupingBy(pod -&gt; pod.getMetadata().getLabels().get(&quot;app&quot;)));&#10;&#10;        for (Map.Entry&lt;String, String&gt; serviceEntry : CORE_SERVICES.entrySet()) {&#10;            String serviceName = serviceEntry.getKey();&#10;            String appLabel = serviceEntry.getValue();&#10;&#10;            List&lt;V1Pod&gt; matchingPods = podsByApp.getOrDefault(appLabel, Collections.emptyList());&#10;&#10;            int desiredReplicas = 0; // We'll infer this from expected counts&#10;            int runningReplicas = 0;&#10;            String status = &quot;Stopped&quot;;&#10;            String podName = &quot;N/A&quot;;&#10;&#10;            if (!matchingPods.isEmpty()) {&#10;                // This is a simple inference. For Deployments/StatefulSets, we'd query them directly for &quot;desiredReplicas&quot;&#10;                // But for a quick health check, counting pods is effective.&#10;                desiredReplicas = (appLabel.equals(&quot;redis-ha&quot;) || appLabel.equals(&quot;rabbitmq&quot;)) ? 3 : 1; // Based on our HA plan&#10;                if (appLabel.equals(&quot;scheduler-service&quot;)) desiredReplicas = 2; // From HA plan&#10;                if (appLabel.equals(&quot;frontend&quot;)) desiredReplicas = 1;&#10;&#10;                runningReplicas = (int) matchingPods.stream()&#10;                        .filter(pod -&gt; &quot;Running&quot;.equals(pod.getStatus().getPhase()))&#10;                        .count();&#10;&#10;                podName = matchingPods.get(0).getMetadata().getName(); // Get first pod name as example&#10;&#10;                if (runningReplicas == 0) {&#10;                    status = &quot;Stopped&quot;;&#10;                } else if (runningReplicas &lt; desiredReplicas) {&#10;                    status = &quot;Degraded&quot;;&#10;                } else {&#10;                    status = &quot;Running&quot;;&#10;                }&#10;            }&#10;&#10;            serviceStatuses.add(new SystemStatusResponse.ServiceStatus(&#10;                    serviceName, status, runningReplicas, desiredReplicas, podName&#10;            ));&#10;        }&#10;        return new SystemStatusResponse(serviceStatuses);&#10;    }&#10;    // --- End of new method ---&#10;&#10;&#10;    // --- processNewDagSubmission (Unchanged) ---&#10;    public String processNewDagSubmission(JsonNode dagPayload) {&#10;        String dagId = &quot;dag-&quot; + UUID.randomUUID();&#10;        try {&#10;            redisTemplate.opsForValue().set(&quot;dag:&quot; + dagId + &quot;:definition&quot;, dagPayload.toString());&#10;            dagPayload.get(&quot;tasks&quot;).forEach(taskNode -&gt; {&#10;                String taskName = taskNode.get(&quot;name&quot;).asText();&#10;                redisTemplate.opsForValue().set(&quot;dag:&quot; + dagId + &quot;:task:&quot; + taskName + &quot;:status&quot;, &quot;PENDING&quot;);&#10;                redisTemplate.delete(String.format(&quot;dag:%s:task:%s:attempts&quot;, dagId, taskName));&#10;                redisTemplate.delete(String.format(&quot;dag:%s:task:%s:logs&quot;, dagId, taskName));&#10;            });&#10;            evaluateDag(dagId);&#10;        } catch (Exception e) {&#10;            LOGGER.error(&quot;Failed to process DAG submission for DAG ID: {}&quot;, dagId, e);&#10;            return null;&#10;        }&#10;        return dagId;&#10;    }&#10;&#10;    // --- evaluateDag (Unchanged) ---&#10;    public void evaluateDag(String dagId) {&#10;        try {&#10;            String dagJson = redisTemplate.opsForValue().get(&quot;dag:&quot; + dagId + &quot;:definition&quot;);&#10;            if (dagJson == null) { return; }&#10;            JsonNode dagPayload = objectMapper.readTree(dagJson);&#10;&#10;            dagPayload.get(&quot;tasks&quot;).forEach(taskNode -&gt; {&#10;                String taskName = taskNode.get(&quot;name&quot;).asText();&#10;                String currentStatus = redisTemplate.opsForValue().get(&quot;dag:&quot; + dagId + &quot;:task:&quot; + taskName + &quot;:status&quot;);&#10;                if (&quot;PENDING&quot;.equals(currentStatus) &amp;&amp; areDependenciesMet(dagId, taskNode)) {&#10;                    dispatchTask(dagId, taskNode);&#10;                }&#10;            });&#10;        } catch (IOException e) {&#10;            LOGGER.error(&quot;Failed to evaluate DAG for ID: {}&quot;, dagId, e);&#10;        }&#10;    }&#10;&#10;    // --- areDependenciesMet (Unchanged) ---&#10;    private boolean areDependenciesMet(String dagId, JsonNode taskNode) {&#10;        if (!taskNode.has(&quot;depends_on&quot;) || taskNode.get(&quot;depends_on&quot;).isEmpty()) {&#10;            return true;&#10;        }&#10;        return StreamSupport.stream(taskNode.get(&quot;depends_on&quot;).spliterator(), false)&#10;                .allMatch(dependencyNode -&gt; &quot;SUCCEEDED&quot;.equals(redisTemplate.opsForValue().get(&quot;dag:&quot; + dagId + &quot;:task:&quot; + dependencyNode.asText() + &quot;:status&quot;)));&#10;    }&#10;&#10;    // --- dispatchTask (v18 pattern - Unchanged) ---&#10;    private void dispatchTask(String dagId, JsonNode taskNode) {&#10;        String taskName = taskNode.get(&quot;name&quot;).asText();&#10;        String jobName = generateK8sJobName(dagId, taskName);&#10;        try {&#10;            redisTemplate.opsForValue().set(&quot;dag:&quot; + dagId + &quot;:task:&quot; + taskName + &quot;:status&quot;, &quot;K8S_JOB_CREATING&quot;);&#10;            String attemptKey = String.format(&quot;dag:%s:task:%s:attempts&quot;, dagId, taskName);&#10;            String currentAttemptStr = redisTemplate.opsForValue().get(attemptKey);&#10;            long nextAttempt = (currentAttemptStr == null) ? 1 : (Long.parseLong(currentAttemptStr) + 1);&#10;            redisTemplate.opsForValue().set(attemptKey, String.valueOf(nextAttempt));&#10;&#10;            V1Job job = createK8sJobDefinition(jobName, dagId, taskNode);&#10;            LOGGER.info(&quot;Submitting Kubernetes Job '{}' for task '{}' (Attempt {})...&quot;, jobName, taskName, nextAttempt);&#10;            V1Job createdJob = batchV1Api.createNamespacedJob(&#10;                    &quot;helios&quot;, job, null, null, null, null&#10;            );&#10;            redisTemplate.opsForValue().set(&quot;dag:&quot; + dagId + &quot;:task:&quot; + taskName + &quot;:status&quot;, &quot;K8S_JOB_SUBMITTED&quot;);&#10;            LOGGER.info(&quot;Successfully submitted Kubernetes Job '{}'.&quot;, createdJob.getMetadata().getName());&#10;&#10;        } catch (ApiException e) {&#10;            LOGGER.error(&quot;Kubernetes API Error dispatching task '{}': {}&quot;, taskName, e.getResponseBody(), e);&#10;            redisTemplate.opsForValue().set(&quot;dag:&quot; + dagId + &quot;:task:&quot; + taskName + &quot;:status&quot;, &quot;DISPATCH_FAILED&quot;);&#10;            handleTaskFailure(dagId, taskName);&#10;        } catch (Exception e) {&#10;            LOGGER.error(&quot;Failed to create/dispatch K8s Job for task '{}'&quot;, taskName, e);&#10;            redisTemplate.opsForValue().set(&quot;dag:&quot; + dagId + &quot;:task:&quot; + taskName + &quot;:status&quot;, &quot;DISPATCH_FAILED&quot;);&#10;            handleTaskFailure(dagId, taskName);&#10;        }&#10;    }&#10;&#10;    // --- handleJobCompletion (Unchanged) ---&#10;    private void handleJobCompletion(V1Job job, boolean succeeded) {&#10;        V1ObjectMeta metadata = job.getMetadata();&#10;        String jobName = metadata.getName();&#10;        String dagId = metadata.getLabels() != null ? metadata.getLabels().get(&quot;helios-dag-id&quot;) : null;&#10;        String taskName = metadata.getLabels() != null ? metadata.getLabels().get(&quot;helios-task-name&quot;) : null;&#10;&#10;        if (dagId == null || taskName == null) {&#10;            LOGGER.error(&quot;Job '{}' completed but missing required helios labels.&quot;, jobName);&#10;            return;&#10;        }&#10;        LOGGER.info(&quot;Processing completion for Job '{}' (Task: '{}', DAG: '{}'). Success={}&quot;, jobName, taskName, dagId, succeeded);&#10;        String logs = fetchPodLogsForJob(jobName, &quot;helios&quot;);&#10;        processJobResult(dagId, taskName, succeeded, logs);&#10;    }&#10;&#10;    // --- fetchPodLogsForJob (v18 pattern - Unchanged) ---&#10;    private String fetchPodLogsForJob(String jobName, String namespace) {&#10;        try {&#10;            String labelSelector = &quot;job-name=&quot; + jobName;&#10;            V1PodList podList = coreV1Api.listNamespacedPod(&#10;                    namespace, null, null, null, null, labelSelector, null, null, null, null, null&#10;            );&#10;&#10;            if (podList != null &amp;&amp; !podList.getItems().isEmpty()) {&#10;                V1Pod pod = podList.getItems().get(0);&#10;                String podName = pod.getMetadata().getName();&#10;                String containerName = (pod.getSpec() != null &amp;&amp; !pod.getSpec().getContainers().isEmpty())&#10;                        ? pod.getSpec().getContainers().get(0).getName() : null;&#10;                if (containerName == null) {&#10;                    return &quot;Error: Container name not found.&quot;;&#10;                }&#10;                String podPhase = pod.getStatus() != null ? pod.getStatus().getPhase() : &quot;Unknown&quot;;&#10;&#10;                if (&quot;Succeeded&quot;.equals(podPhase) || &quot;Failed&quot;.equals(podPhase)) {&#10;                    String logContent = coreV1Api.readNamespacedPodLog(&#10;                            podName,                          // name&#10;                            namespace,                        // namespace&#10;                            containerName,                    // container&#10;                            Boolean.FALSE,                    // follow&#10;                            Boolean.FALSE,                    // insecureSkipTLSVerifyBackend or pretty (per v18)&#10;                            (Integer) null,                   // limitBytes&#10;                            (String) null,                    // sinceTime&#10;                            Boolean.FALSE,                    // timestamps&#10;                            (Integer) null,                   // sinceSeconds&#10;                            (Integer) null,                   // tailLines&#10;                            Boolean.FALSE                     // limitBytes? or some boolean flag (per v18)&#10;                    );&#10;                    return logContent != null ? logContent : &quot;&quot;;&#10;                } else {&#10;                    return &quot;Pod logs not ready (Phase: &quot; + podPhase + &quot;)&quot;;&#10;                }&#10;            } else {&#10;                return &quot;Error: Pod not found.&quot;;&#10;            }&#10;        } catch (ApiException e) {&#10;            if (e.getCode() == 404) { return &quot;Error: Pod/Job not found.&quot;; }&#10;            LOGGER.error(&quot;K8s API Error fetching logs for Job '{}': {}&quot;, jobName, e.getResponseBody(), e);&#10;            return &quot;Error fetching logs: API Error &quot; + e.getCode();&#10;        } catch (Exception e) {&#10;            LOGGER.error(&quot;Unexpected error fetching logs for Job '{}'&quot;, jobName, e);&#10;            return &quot;Error fetching logs: &quot; + e.getMessage();&#10;        }&#10;    }&#10;&#10;&#10;    // --- processJobResult (Refined logic - Unchanged) ---&#10;    private void processJobResult(String dagId, String taskName, boolean success, String logs) {&#10;        try {&#10;            String taskStatusKey = String.format(&quot;dag:%s:task:%s:status&quot;, dagId, taskName);&#10;            String taskLogsKey = String.format(&quot;dag:%s:task:%s:logs&quot;, dagId, taskName);&#10;            List&lt;String&gt; logList = (logs != null &amp;&amp; !logs.startsWith(&quot;Error:&quot;)) ? List.of(logs.split(&quot;\n&quot;)) : List.of(logs);&#10;            redisTemplate.opsForValue().set(taskLogsKey, objectMapper.writeValueAsString(logList));&#10;            String currentStatus = redisTemplate.opsForValue().get(taskStatusKey);&#10;            if (&quot;SUCCEEDED&quot;.equals(currentStatus) || &quot;FAILED&quot;.equals(currentStatus) || &quot;UPSTREAM_FAILED&quot;.equals(currentStatus)) {&#10;                LOGGER.warn(&quot;Task '{}' already in final state '{}'. Ignoring job completion event.&quot;, taskName, currentStatus);&#10;                return;&#10;            }&#10;            if (success) {&#10;                redisTemplate.opsForValue().set(taskStatusKey, &quot;SUCCEEDED&quot;);&#10;                LOGGER.info(&quot;Task '{}' SUCCEEDED. Triggering DAG re-evaluation.&quot;, taskName);&#10;                evaluateDag(dagId);&#10;            } else {&#10;                handleTaskFailure(dagId, taskName);&#10;            }&#10;        } catch (Exception e) {&#10;            LOGGER.error(&quot;CRITICAL: Failed to process final job result for task '{}', DAG '{}'.&quot;, taskName, dagId, e);&#10;        }&#10;    }&#10;&#10;&#10;    // --- handleTaskFailure (Refined logic - Unchanged) ---&#10;    private void handleTaskFailure(String dagId, String taskName) {&#10;        try {&#10;            String dagJson = redisTemplate.opsForValue().get(&quot;dag:&quot; + dagId + &quot;:definition&quot;);&#10;            if (dagJson == null) { return; }&#10;            JsonNode dagPayload = objectMapper.readTree(dagJson);&#10;            JsonNode taskNode = findTaskNode(dagPayload, taskName);&#10;            if (taskNode == null) { return; }&#10;&#10;            int maxRetries = taskNode.path(&quot;retries&quot;).path(&quot;count&quot;).asInt(0);&#10;            String attemptKey = String.format(&quot;dag:%s:task:%s:attempts&quot;, dagId, taskName);&#10;            long attemptsMade = redisTemplate.opsForValue().increment(attemptKey); // Increments and returns the new value&#10;&#10;            if (attemptsMade &lt;= maxRetries + 1) { // We check &lt;= maxRetries + 1 because the first attempt is 1&#10;                if(attemptsMade &lt;= maxRetries) { // This means we have retries left&#10;                    LOGGER.warn(&quot;Task '{}' FAILED on attempt {}. Re-dispatching for retry... (Max retries: {})&quot;, taskName, attemptsMade, maxRetries);&#10;                    dispatchTask(dagId, taskNode);&#10;                } else { // This means attemptsMade == maxRetries + 1, which was the final attempt&#10;                    LOGGER.error(&quot;Task '{}' FAILED on final attempt {}. Initiating failure propagation.&quot;, taskName, attemptsMade -1); // Log the attempt number that failed&#10;                    redisTemplate.opsForValue().set(String.format(&quot;dag:%s:task:%s:status&quot;, dagId, taskName), &quot;FAILED&quot;);&#10;                    propagateFailure(dagId, taskName);&#10;                }&#10;            }&#10;        } catch (Exception e) {&#10;            LOGGER.error(&quot;Unexpected error during failure handling for task '{}', DAG '{}'&quot;, taskName, dagId, e);&#10;        }&#10;    }&#10;&#10;    // --- propagateFailure (Refined logic - Unchanged) ---&#10;    public void propagateFailure(String dagId, String failedTaskName) {&#10;        try {&#10;            String dagJson = redisTemplate.opsForValue().get(&quot;dag:&quot; + dagId + &quot;:definition&quot;);&#10;            if (dagJson == null) { return; }&#10;            JsonNode dagPayload = objectMapper.readTree(dagJson);&#10;            dagPayload.get(&quot;tasks&quot;).forEach(taskNode -&gt; {&#10;                if (taskNode.has(&quot;depends_on&quot;)) {&#10;                    taskNode.get(&quot;depends_on&quot;).forEach(depNode -&gt; {&#10;                        if (depNode.asText().equals(failedTaskName)) {&#10;                            String childTaskName = taskNode.get(&quot;name&quot;).asText();&#10;                            String childStatusKey = String.format(&quot;dag:%s:task:%s:status&quot;, dagId, childTaskName);&#10;                            String currentChildStatus = redisTemplate.opsForValue().get(childStatusKey);&#10;                            if (&quot;PENDING&quot;.equals(currentChildStatus)) {&#10;                                redisTemplate.opsForValue().set(childStatusKey, &quot;UPSTREAM_FAILED&quot;);&#10;                                LOGGER.warn(&quot;Propagating failure: Task '{}' marked as UPSTREAM_FAILED.&quot;, childTaskName);&#10;                                propagateFailure(dagId, childTaskName);&#10;                            }&#10;                        }&#10;                    });&#10;                }&#10;            });&#10;        } catch (IOException e) {&#10;            LOGGER.error(&quot;Failed to read DAG definition during failure propagation for DAG ID: {}&quot;, dagId, e);&#10;        }&#10;    }&#10;&#10;    // --- getDagStatus (Unchanged) ---&#10;    public DagStatusResponse getDagStatus(String dagId) {&#10;        try {&#10;            String dagJson = redisTemplate.opsForValue().get(&quot;dag:&quot; + dagId + &quot;:definition&quot;);&#10;            if (dagJson == null) { return null; }&#10;            JsonNode dagPayload = objectMapper.readTree(dagJson);&#10;            String dagName = dagPayload.get(&quot;dagName&quot;).asText();&#10;            List&lt;DagStatusResponse.TaskStatus&gt; taskStatuses = new ArrayList&lt;&gt;();&#10;&#10;            for (JsonNode taskNode : dagPayload.get(&quot;tasks&quot;)) {&#10;                String taskName = taskNode.get(&quot;name&quot;).asText();&#10;                String status = redisTemplate.opsForValue().get(String.format(&quot;dag:%s:task:%s:status&quot;, dagId, taskName));&#10;                status = (status == null) ? &quot;PENDING&quot; : status;&#10;                String logsJson = redisTemplate.opsForValue().get(String.format(&quot;dag:%s:task:%s:logs&quot;, dagId, taskName));&#10;                List&lt;String&gt; logs = (logsJson != null) ? objectMapper.readValue(logsJson, new TypeReference&lt;&gt;() {}) : Collections.emptyList();&#10;                List&lt;String&gt; dependsOn = new ArrayList&lt;&gt;();&#10;                if (taskNode.has(&quot;depends_on&quot;)) {&#10;                    for (JsonNode depNode : taskNode.get(&quot;depends_on&quot;)) { dependsOn.add(depNode.asText()); }&#10;                }&#10;                taskStatuses.add(new DagStatusResponse.TaskStatus(taskName, status, dependsOn, logs));&#10;            }&#10;            return new DagStatusResponse(dagId, dagName, taskStatuses);&#10;        } catch (IOException e) {&#10;            LOGGER.error(&quot;Failed to construct DAG status for ID: {}&quot;, dagId, e);&#10;            return null;&#10;        }&#10;    }&#10;&#10;&#10;    // --- HELPER METHODS (Unchanged) ---&#10;    private JsonNode findTaskNode(JsonNode dagPayload, String taskName) {&#10;        for (JsonNode task : dagPayload.get(&quot;tasks&quot;)) {&#10;            if (task.get(&quot;name&quot;).asText().equals(taskName)) { return task; }&#10;        }&#10;        return null;&#10;    }&#10;&#10;    private String generateK8sJobName(String dagId, String taskName) {&#10;        String cleanDagId = dagId.replaceAll(&quot;[^a-z0-9-]&quot;, &quot;&quot;).toLowerCase();&#10;        String cleanTaskName = taskName.replaceAll(&quot;[^a-z0-9-]&quot;, &quot;&quot;).toLowerCase();&#10;        String base = String.format(&quot;helios-%s-%s&quot;,&#10;                cleanDagId.substring(Math.max(0, cleanDagId.length() - 8)),&#10;                cleanTaskName);&#10;        base = base.substring(0, Math.min(base.length(), 45)).replaceAll(&quot;-$&quot;, &quot;&quot;);&#10;        return base + &quot;-&quot; + Long.toString(System.nanoTime() % 100000, 36);&#10;    }&#10;&#10;    private V1Job createK8sJobDefinition(String jobName, String dagId, JsonNode taskNode) throws IOException {&#10;        String image = taskNode.get(&quot;image&quot;).asText();&#10;        List&lt;String&gt; command = objectMapper.convertValue(taskNode.get(&quot;command&quot;), new TypeReference&lt;List&lt;String&gt;&gt;() {});&#10;        String taskName = taskNode.get(&quot;name&quot;).asText();&#10;        String cleanTaskNameK8s = taskName.replaceAll(&quot;[^A-Za-z0-9\\-_.]&quot;, &quot;&quot;).toLowerCase();&#10;        cleanTaskNameK8s = cleanTaskNameK8s.substring(0, Math.min(cleanTaskNameK8s.length(), 63)).replaceAll(&quot;^-|-$&quot;, &quot;&quot;);&#10;        String cleanDagIdK8s = dagId.replaceAll(&quot;[^A-Za-z0-9\\-_.]&quot;, &quot;&quot;).toLowerCase();&#10;        cleanDagIdK8s = cleanDagIdK8s.substring(0, Math.min(cleanDagIdK8s.length(), 63)).replaceAll(&quot;^-|-$&quot;, &quot;&quot;);&#10;&#10;        V1ObjectMeta jobMeta = new V1ObjectMeta()&#10;                .name(jobName)&#10;                .namespace(&quot;helios&quot;)&#10;                .putLabelsItem(&quot;app&quot;, &quot;helios-task&quot;)&#10;                .putLabelsItem(&quot;helios-dag-id&quot;, cleanDagIdK8s)&#10;                .putLabelsItem(&quot;helios-task-name&quot;, cleanTaskNameK8s);&#10;&#10;        V1Container container = new V1Container()&#10;                .name(cleanTaskNameK8s.substring(0, Math.min(cleanTaskNameK8s.length(), 50)) + &quot;-cont&quot;)&#10;                .image(image)&#10;                .command(command);&#10;&#10;        V1PodSpec podSpec = new V1PodSpec()&#10;                .restartPolicy(&quot;Never&quot;)&#10;                .addContainersItem(container);&#10;&#10;        V1PodTemplateSpec template = new V1PodTemplateSpec()&#10;                .metadata(new V1ObjectMeta()&#10;                        .putLabelsItem(&quot;app&quot;, &quot;helios-task-pod&quot;)&#10;                        .putLabelsItem(&quot;job-name&quot;, jobName)&#10;                        .putLabelsItem(&quot;helios-dag-id&quot;, cleanDagIdK8s)&#10;                        .putLabelsItem(&quot;helios-task-name&quot;, cleanTaskNameK8s))&#10;                .spec(podSpec);&#10;&#10;        V1JobSpec jobSpec = new V1JobSpec()&#10;                .ttlSecondsAfterFinished(3600)&#10;                .backoffLimit(0)&#10;                .template(template);&#10;&#10;        return new V1Job()&#10;                .apiVersion(&quot;batch/v1&quot;)&#10;                .kind(&quot;Job&quot;)&#10;                .metadata(jobMeta)&#10;                .spec(jobSpec);&#10;    }&#10;}&#10;&#10;" />
              <option name="updatedContent" value="package com.example.demo.service;&#10;&#10;// All existing imports from your v18 file...&#10;import com.example.demo.config.RabbitMQConfig;&#10;import com.example.demo.dto.DagStatusResponse;&#10;import com.example.demo.dto.SystemStatusResponse; // NEW Import&#10;import com.fasterxml.jackson.core.type.TypeReference;&#10;import com.fasterxml.jackson.databind.JsonNode;&#10;import com.fasterxml.jackson.databind.ObjectMapper;&#10;import org.slf4j.Logger;&#10;import org.slf4j.LoggerFactory;&#10;import org.springframework.beans.factory.annotation.Autowired;&#10;import org.springframework.data.redis.core.StringRedisTemplate;&#10;import org.springframework.stereotype.Service;&#10;&#10;import java.io.IOException;&#10;import java.io.InputStream;&#10;import java.util.ArrayList;&#10;import java.util.Arrays; // NEW Import&#10;import java.util.Collections;&#10;import java.util.List;&#10;import java.util.Map; // NEW Import&#10;import java.util.UUID;&#10;import java.util.stream.Collectors; // NEW Import&#10;import java.util.stream.StreamSupport;&#10;&#10;// Kubernetes Imports (v17.x)&#10;import io.kubernetes.client.openapi.ApiClient;&#10;import io.kubernetes.client.openapi.ApiException;&#10;import io.kubernetes.client.openapi.Configuration;&#10;import io.kubernetes.client.openapi.apis.AppsV1Api;&#10;import io.kubernetes.client.openapi.apis.BatchV1Api;&#10;import io.kubernetes.client.openapi.apis.CoreV1Api;&#10;import io.kubernetes.client.openapi.models.*; // Keep model imports&#10;import io.kubernetes.client.util.ClientBuilder;&#10;import io.kubernetes.client.util.Streams;&#10;import io.kubernetes.client.informer.SharedInformerFactory;&#10;import io.kubernetes.client.informer.cache.Indexer;&#10;import io.kubernetes.client.informer.ResourceEventHandler;&#10;import io.kubernetes.client.util.CallGeneratorParams;&#10;import okhttp3.Call;&#10;// import io.kubernetes.client.util.generic.GenericKubernetesApi; // This was from v19+, v18 uses direct call&#10;&#10;import jakarta.annotation.PostConstruct;&#10;import jakarta.annotation.PreDestroy;&#10;&#10;/**&#10; * The OrchestratorService is the &quot;brain&quot; managing DAG execution via Kubernetes Jobs (v18.0.0 client).&#10; */&#10;@Service&#10;public class OrchestratorService {&#10;&#10;    private static final Logger LOGGER = LoggerFactory.getLogger(OrchestratorService.class);&#10;    private final StringRedisTemplate redisTemplate;&#10;    private final ObjectMapper objectMapper;&#10;    private final BatchV1Api batchV1Api;&#10;    private final CoreV1Api coreV1Api;&#10;    private final AppsV1Api appsV1Api;&#10;    private final SharedInformerFactory informerFactory;&#10;    private Indexer&lt;V1Job&gt; jobIndexer;&#10;&#10;    // Map of our core components and their app labels in Kubernetes&#10;    private static final Map&lt;String, String&gt; CORE_SERVICES = Map.of(&#10;            &quot;Scheduler&quot;, &quot;scheduler-service&quot;,&#10;            &quot;Frontend&quot;, &quot;frontend&quot;,&#10;            &quot;Redis&quot;, &quot;redis&quot;, // fixed label to match manifest&#10;            &quot;RabbitMQ&quot;, &quot;rabbitmq&quot;&#10;    );&#10;&#10;&#10;    @Autowired&#10;    public OrchestratorService(StringRedisTemplate redisTemplate, ObjectMapper objectMapper) throws IOException {&#10;        this.redisTemplate = redisTemplate;&#10;        this.objectMapper = objectMapper;&#10;&#10;        ApiClient client = ClientBuilder.cluster().build();&#10;        client.setReadTimeout(0);&#10;        client.setWriteTimeout(0);&#10;        client.setConnectTimeout(0);&#10;        Configuration.setDefaultApiClient(client);&#10;        this.batchV1Api = new BatchV1Api(client);&#10;        this.coreV1Api = new CoreV1Api(client);&#10;        this.appsV1Api = new AppsV1Api(client);&#10;        this.informerFactory = new SharedInformerFactory(client);&#10;&#10;        setupJobWatcher();&#10;&#10;        LOGGER.info(&quot;Kubernetes client and Job watcher initialized successfully.&quot;);&#10;    }&#10;&#10;    // --- Configure the Kubernetes Job Watcher (Corrected for v18.0.0 API Call Signature) ---&#10;    private void setupJobWatcher() {&#10;        var jobInformer = informerFactory.sharedIndexInformerFor(&#10;                (CallGeneratorParams params) -&gt; {&#10;                    try {&#10;                        return batchV1Api.listNamespacedJobCall(&#10;                                &quot;helios&quot;, // namespace&#10;                                null, // pretty&#10;                                null, // allowWatchBookmarks&#10;                                null, // _continue&#10;                                null, // fieldSelector&#10;                                &quot;app=helios-task&quot;, // labelSelector to only watch our jobs&#10;                                null, // limit&#10;                                params.resourceVersion, // resourceVersion from params&#10;                                null, // resourceVersionMatch&#10;                                params.timeoutSeconds, // timeoutSeconds from params&#10;                                params.watch, // watch from params&#10;                                null // _callback&#10;                        );&#10;                    } catch (ApiException e) {&#10;                        LOGGER.error(&quot;Watcher: Failed to create list call for Jobs: {}&quot;, e.getResponseBody(), e);&#10;                        throw new RuntimeException(e);&#10;                    }&#10;                },&#10;                V1Job.class, V1JobList.class&#10;        );&#10;&#10;        // --- Event Handler logic (Unchanged) ---&#10;        jobInformer.addEventHandler(new ResourceEventHandler&lt;V1Job&gt;() {&#10;            @Override public void onAdd(V1Job job) { /* Ignore */ }&#10;            @Override&#10;            public void onUpdate(V1Job oldJob, V1Job newJob) {&#10;                // ... (existing watcher logic as before) ...&#10;                String jobName = newJob.getMetadata().getName();&#10;                LOGGER.debug(&quot;Watcher: Job updated - {}&quot;, jobName);&#10;                var status = newJob.getStatus();&#10;                if (status != null) {&#10;                    boolean succeeded = status.getSucceeded() != null &amp;&amp; status.getSucceeded() &gt; 0;&#10;                    boolean failed = status.getFailed() != null &amp;&amp; status.getFailed() &gt; 0;&#10;                    if (succeeded || failed) {&#10;                        String processedKey = &quot;job:processed:&quot; + jobName;&#10;                        if (Boolean.TRUE.equals(redisTemplate.opsForValue().setIfAbsent(processedKey, &quot;true&quot;))) {&#10;                            redisTemplate.expire(processedKey, java.time.Duration.ofHours(2));&#10;                            LOGGER.info(&quot;Watcher: Job '{}' completed (Succeeded={}, Failed={}). Processing result...&quot;, jobName, succeeded, failed);&#10;                            handleJobCompletion(newJob, succeeded);&#10;                        } else {&#10;                            LOGGER.debug(&quot;Watcher: Job '{}' completion already processed.&quot;, jobName);&#10;                        }&#10;                    }&#10;                }&#10;            }&#10;            @Override public void onDelete(V1Job job, boolean deletedFinalStateUnknown) { /* Ignore */ }&#10;        });&#10;        this.jobIndexer = jobInformer.getIndexer();&#10;    }&#10;&#10;    // --- Start/Stop watchers (Unchanged) ---&#10;    @PostConstruct&#10;    public void startWatchers() {&#10;        LOGGER.info(&quot;Starting Kubernetes watchers...&quot;);&#10;        informerFactory.startAllRegisteredInformers();&#10;        LOGGER.info(&quot;Kubernetes watchers started.&quot;);&#10;    }&#10;    @PreDestroy&#10;    public void stopWatchers() {&#10;        LOGGER.info(&quot;Stopping Kubernetes watchers...&quot;);&#10;        informerFactory.stopAllRegisteredInformers(true); // Graceful shutdown&#10;        LOGGER.info(&quot;Kubernetes watchers stopped.&quot;);&#10;    }&#10;&#10;    // --- NEW METHOD: Get System Health Status ---&#10;    public SystemStatusResponse getSystemStatus() throws ApiException {&#10;        List&lt;SystemStatusResponse.ServiceStatus&gt; serviceStatuses = new ArrayList&lt;&gt;();&#10;        String namespace = &quot;helios&quot;;&#10;&#10;        V1PodList allPods = coreV1Api.listNamespacedPod(&#10;                namespace, null, null, null, null, null, null, null, null, null, null&#10;        );&#10;        Map&lt;String, List&lt;V1Pod&gt;&gt; podsByApp = allPods.getItems().stream()&#10;                .filter(pod -&gt; pod.getMetadata() != null &amp;&amp; pod.getMetadata().getLabels() != null)&#10;                .collect(Collectors.groupingBy(pod -&gt; pod.getMetadata().getLabels().get(&quot;app&quot;)));&#10;&#10;        for (Map.Entry&lt;String, String&gt; serviceEntry : CORE_SERVICES.entrySet()) {&#10;            String serviceName = serviceEntry.getKey();&#10;            String appLabel = serviceEntry.getValue();&#10;&#10;            List&lt;V1Pod&gt; matchingPods = podsByApp.getOrDefault(appLabel, Collections.emptyList());&#10;&#10;            // Derive desired replicas from Deployments/StatefulSets with the same app label&#10;            int desiredReplicas = 0;&#10;            try {&#10;                String labelSelector = &quot;app=&quot; + appLabel;&#10;                V1DeploymentList dpls = appsV1Api.listNamespacedDeployment(&#10;                        namespace, null, null, null, null, labelSelector, null, null, null, null, null&#10;                );&#10;                if (dpls != null &amp;&amp; dpls.getItems() != null) {&#10;                    desiredReplicas += dpls.getItems().stream()&#10;                            .map(d -&gt; d.getSpec() != null &amp;&amp; d.getSpec().getReplicas() != null ? d.getSpec().getReplicas() : 0)&#10;                            .reduce(0, Integer::sum);&#10;                }&#10;                V1StatefulSetList stss = appsV1Api.listNamespacedStatefulSet(&#10;                        namespace, null, null, null, null, labelSelector, null, null, null, null, null&#10;                );&#10;                if (stss != null &amp;&amp; stss.getItems() != null) {&#10;                    desiredReplicas += stss.getItems().stream()&#10;                            .map(s -&gt; s.getSpec() != null &amp;&amp; s.getSpec().getReplicas() != null ? s.getSpec().getReplicas() : 0)&#10;                            .reduce(0, Integer::sum);&#10;                }&#10;            } catch (ApiException e) {&#10;                // Fall back if Apps API is not accessible&#10;                desiredReplicas = 0;&#10;            }&#10;&#10;            // Fallbacks: if controller info not found, infer from pods&#10;            if (desiredReplicas == 0) {&#10;                desiredReplicas = matchingPods.isEmpty() ? 0 : 1;&#10;            }&#10;&#10;            int runningReplicas = (int) matchingPods.stream()&#10;                    .filter(pod -&gt; &quot;Running&quot;.equals(pod.getStatus() != null ? pod.getStatus().getPhase() : null))&#10;                    .count();&#10;&#10;            String status;&#10;            if (desiredReplicas == 0) {&#10;                status = runningReplicas &gt; 0 ? &quot;Running&quot; : &quot;Stopped&quot;;&#10;            } else if (runningReplicas == 0) {&#10;                status = &quot;Stopped&quot;;&#10;            } else if (runningReplicas &lt; desiredReplicas) {&#10;                status = &quot;Degraded&quot;;&#10;            } else {&#10;                status = &quot;Running&quot;;&#10;            }&#10;&#10;            String podName = matchingPods.isEmpty() ? &quot;N/A&quot; : matchingPods.get(0).getMetadata().getName();&#10;&#10;            serviceStatuses.add(new SystemStatusResponse.ServiceStatus(&#10;                    serviceName, status, runningReplicas, desiredReplicas, podName&#10;            ));&#10;        }&#10;        return new SystemStatusResponse(serviceStatuses);&#10;    }&#10;    // --- End of new method ---&#10;&#10;&#10;    // --- processNewDagSubmission (Unchanged) ---&#10;    public String processNewDagSubmission(JsonNode dagPayload) {&#10;        String dagId = &quot;dag-&quot; + UUID.randomUUID();&#10;        try {&#10;            redisTemplate.opsForValue().set(&quot;dag:&quot; + dagId + &quot;:definition&quot;, dagPayload.toString());&#10;            dagPayload.get(&quot;tasks&quot;).forEach(taskNode -&gt; {&#10;                String taskName = taskNode.get(&quot;name&quot;).asText();&#10;                redisTemplate.opsForValue().set(&quot;dag:&quot; + dagId + &quot;:task:&quot; + taskName + &quot;:status&quot;, &quot;PENDING&quot;);&#10;                redisTemplate.delete(String.format(&quot;dag:%s:task:%s:attempts&quot;, dagId, taskName));&#10;                redisTemplate.delete(String.format(&quot;dag:%s:task:%s:logs&quot;, dagId, taskName));&#10;            });&#10;            evaluateDag(dagId);&#10;        } catch (Exception e) {&#10;            LOGGER.error(&quot;Failed to process DAG submission for DAG ID: {}&quot;, dagId, e);&#10;            return null;&#10;        }&#10;        return dagId;&#10;    }&#10;&#10;    // --- evaluateDag (Unchanged) ---&#10;    public void evaluateDag(String dagId) {&#10;        try {&#10;            String dagJson = redisTemplate.opsForValue().get(&quot;dag:&quot; + dagId + &quot;:definition&quot;);&#10;            if (dagJson == null) { return; }&#10;            JsonNode dagPayload = objectMapper.readTree(dagJson);&#10;&#10;            dagPayload.get(&quot;tasks&quot;).forEach(taskNode -&gt; {&#10;                String taskName = taskNode.get(&quot;name&quot;).asText();&#10;                String currentStatus = redisTemplate.opsForValue().get(&quot;dag:&quot; + dagId + &quot;:task:&quot; + taskName + &quot;:status&quot;);&#10;                if (&quot;PENDING&quot;.equals(currentStatus) &amp;&amp; areDependenciesMet(dagId, taskNode)) {&#10;                    dispatchTask(dagId, taskNode);&#10;                }&#10;            });&#10;        } catch (IOException e) {&#10;            LOGGER.error(&quot;Failed to evaluate DAG for ID: {}&quot;, dagId, e);&#10;        }&#10;    }&#10;&#10;    // --- areDependenciesMet (Unchanged) ---&#10;    private boolean areDependenciesMet(String dagId, JsonNode taskNode) {&#10;        if (!taskNode.has(&quot;depends_on&quot;) || taskNode.get(&quot;depends_on&quot;).isEmpty()) {&#10;            return true;&#10;        }&#10;        return StreamSupport.stream(taskNode.get(&quot;depends_on&quot;).spliterator(), false)&#10;                .allMatch(dependencyNode -&gt; &quot;SUCCEEDED&quot;.equals(redisTemplate.opsForValue().get(&quot;dag:&quot; + dagId + &quot;:task:&quot; + dependencyNode.asText() + &quot;:status&quot;)));&#10;    }&#10;&#10;    // --- dispatchTask (v18 pattern - Unchanged) ---&#10;    private void dispatchTask(String dagId, JsonNode taskNode) {&#10;        String taskName = taskNode.get(&quot;name&quot;).asText();&#10;        String jobName = generateK8sJobName(dagId, taskName);&#10;        try {&#10;            redisTemplate.opsForValue().set(&quot;dag:&quot; + dagId + &quot;:task:&quot; + taskName + &quot;:status&quot;, &quot;K8S_JOB_CREATING&quot;);&#10;            String attemptKey = String.format(&quot;dag:%s:task:%s:attempts&quot;, dagId, taskName);&#10;            String currentAttemptStr = redisTemplate.opsForValue().get(attemptKey);&#10;            long nextAttempt = (currentAttemptStr == null) ? 1 : (Long.parseLong(currentAttemptStr) + 1);&#10;            redisTemplate.opsForValue().set(attemptKey, String.valueOf(nextAttempt));&#10;&#10;            V1Job job = createK8sJobDefinition(jobName, dagId, taskNode);&#10;            LOGGER.info(&quot;Submitting Kubernetes Job '{}' for task '{}' (Attempt {})...&quot;, jobName, taskName, nextAttempt);&#10;            V1Job createdJob = batchV1Api.createNamespacedJob(&#10;                    &quot;helios&quot;, job, null, null, null, null&#10;            );&#10;            redisTemplate.opsForValue().set(&quot;dag:&quot; + dagId + &quot;:task:&quot; + taskName + &quot;:status&quot;, &quot;K8S_JOB_SUBMITTED&quot;);&#10;            LOGGER.info(&quot;Successfully submitted Kubernetes Job '{}'.&quot;, createdJob.getMetadata().getName());&#10;&#10;        } catch (ApiException e) {&#10;            LOGGER.error(&quot;Kubernetes API Error dispatching task '{}': {}&quot;, taskName, e.getResponseBody(), e);&#10;            redisTemplate.opsForValue().set(&quot;dag:&quot; + dagId + &quot;:task:&quot; + taskName + &quot;:status&quot;, &quot;DISPATCH_FAILED&quot;);&#10;            handleTaskFailure(dagId, taskName);&#10;        } catch (Exception e) {&#10;            LOGGER.error(&quot;Failed to create/dispatch K8s Job for task '{}'&quot;, taskName, e);&#10;            redisTemplate.opsForValue().set(&quot;dag:&quot; + dagId + &quot;:task:&quot; + taskName + &quot;:status&quot;, &quot;DISPATCH_FAILED&quot;);&#10;            handleTaskFailure(dagId, taskName);&#10;        }&#10;    }&#10;&#10;    // --- handleJobCompletion (Unchanged) ---&#10;    private void handleJobCompletion(V1Job job, boolean succeeded) {&#10;        V1ObjectMeta metadata = job.getMetadata();&#10;        String jobName = metadata.getName();&#10;        String dagId = metadata.getLabels() != null ? metadata.getLabels().get(&quot;helios-dag-id&quot;) : null;&#10;        String taskName = metadata.getLabels() != null ? metadata.getLabels().get(&quot;helios-task-name&quot;) : null;&#10;&#10;        if (dagId == null || taskName == null) {&#10;            LOGGER.error(&quot;Job '{}' completed but missing required helios labels.&quot;, jobName);&#10;            return;&#10;        }&#10;        LOGGER.info(&quot;Processing completion for Job '{}' (Task: '{}', DAG: '{}'). Success={}&quot;, jobName, taskName, dagId, succeeded);&#10;        String logs = fetchPodLogsForJob(jobName, &quot;helios&quot;);&#10;        processJobResult(dagId, taskName, succeeded, logs);&#10;    }&#10;&#10;    // --- fetchPodLogsForJob (v18 pattern - Unchanged) ---&#10;    private String fetchPodLogsForJob(String jobName, String namespace) {&#10;        try {&#10;            String labelSelector = &quot;job-name=&quot; + jobName;&#10;            V1PodList podList = coreV1Api.listNamespacedPod(&#10;                    namespace, null, null, null, null, labelSelector, null, null, null, null, null&#10;            );&#10;&#10;            if (podList != null &amp;&amp; !podList.getItems().isEmpty()) {&#10;                V1Pod pod = podList.getItems().get(0);&#10;                String podName = pod.getMetadata().getName();&#10;                String containerName = (pod.getSpec() != null &amp;&amp; !pod.getSpec().getContainers().isEmpty())&#10;                        ? pod.getSpec().getContainers().get(0).getName() : null;&#10;                if (containerName == null) {&#10;                    return &quot;Error: Container name not found.&quot;;&#10;                }&#10;                String podPhase = pod.getStatus() != null ? pod.getStatus().getPhase() : &quot;Unknown&quot;;&#10;&#10;                if (&quot;Succeeded&quot;.equals(podPhase) || &quot;Failed&quot;.equals(podPhase)) {&#10;                    String logContent = coreV1Api.readNamespacedPodLog(&#10;                            podName,                          // name&#10;                            namespace,                        // namespace&#10;                            containerName,                    // container&#10;                            Boolean.FALSE,                    // follow&#10;                            Boolean.FALSE,                    // insecureSkipTLSVerifyBackend or pretty (per v18)&#10;                            (Integer) null,                   // limitBytes&#10;                            (String) null,                    // sinceTime&#10;                            Boolean.FALSE,                    // timestamps&#10;                            (Integer) null,                   // sinceSeconds&#10;                            (Integer) null,                   // tailLines&#10;                            Boolean.FALSE                     // limitBytes? or some boolean flag (per v18)&#10;                    );&#10;                    return logContent != null ? logContent : &quot;&quot;;&#10;                } else {&#10;                    return &quot;Pod logs not ready (Phase: &quot; + podPhase + &quot;)&quot;;&#10;                }&#10;            } else {&#10;                return &quot;Error: Pod not found.&quot;;&#10;            }&#10;        } catch (ApiException e) {&#10;            if (e.getCode() == 404) { return &quot;Error: Pod/Job not found.&quot;; }&#10;            LOGGER.error(&quot;K8s API Error fetching logs for Job '{}': {}&quot;, jobName, e.getResponseBody(), e);&#10;            return &quot;Error fetching logs: API Error &quot; + e.getCode();&#10;        } catch (Exception e) {&#10;            LOGGER.error(&quot;Unexpected error fetching logs for Job '{}'&quot;, jobName, e);&#10;            return &quot;Error fetching logs: &quot; + e.getMessage();&#10;        }&#10;    }&#10;&#10;&#10;    // --- processJobResult (Refined logic - Unchanged) ---&#10;    private void processJobResult(String dagId, String taskName, boolean success, String logs) {&#10;        try {&#10;            String taskStatusKey = String.format(&quot;dag:%s:task:%s:status&quot;, dagId, taskName);&#10;            String taskLogsKey = String.format(&quot;dag:%s:task:%s:logs&quot;, dagId, taskName);&#10;            List&lt;String&gt; logList = (logs != null &amp;&amp; !logs.startsWith(&quot;Error:&quot;)) ? List.of(logs.split(&quot;\n&quot;)) : List.of(logs);&#10;            redisTemplate.opsForValue().set(taskLogsKey, objectMapper.writeValueAsString(logList));&#10;            String currentStatus = redisTemplate.opsForValue().get(taskStatusKey);&#10;            if (&quot;SUCCEEDED&quot;.equals(currentStatus) || &quot;FAILED&quot;.equals(currentStatus) || &quot;UPSTREAM_FAILED&quot;.equals(currentStatus)) {&#10;                LOGGER.warn(&quot;Task '{}' already in final state '{}'. Ignoring job completion event.&quot;, taskName, currentStatus);&#10;                return;&#10;            }&#10;            if (success) {&#10;                redisTemplate.opsForValue().set(taskStatusKey, &quot;SUCCEEDED&quot;);&#10;                LOGGER.info(&quot;Task '{}' SUCCEEDED. Triggering DAG re-evaluation.&quot;, taskName);&#10;                evaluateDag(dagId);&#10;            } else {&#10;                handleTaskFailure(dagId, taskName);&#10;            }&#10;        } catch (Exception e) {&#10;            LOGGER.error(&quot;CRITICAL: Failed to process final job result for task '{}', DAG '{}'.&quot;, taskName, dagId, e);&#10;        }&#10;    }&#10;&#10;&#10;    // --- handleTaskFailure (Refined logic - Unchanged) ---&#10;    private void handleTaskFailure(String dagId, String taskName) {&#10;        try {&#10;            String dagJson = redisTemplate.opsForValue().get(&quot;dag:&quot; + dagId + &quot;:definition&quot;);&#10;            if (dagJson == null) { return; }&#10;            JsonNode dagPayload = objectMapper.readTree(dagJson);&#10;            JsonNode taskNode = findTaskNode(dagPayload, taskName);&#10;            if (taskNode == null) { return; }&#10;&#10;            int maxRetries = taskNode.path(&quot;retries&quot;).path(&quot;count&quot;).asInt(0);&#10;            String attemptKey = String.format(&quot;dag:%s:task:%s:attempts&quot;, dagId, taskName);&#10;            long attemptsMade = redisTemplate.opsForValue().increment(attemptKey); // Increments and returns the new value&#10;&#10;            if (attemptsMade &lt;= maxRetries + 1) { // We check &lt;= maxRetries + 1 because the first attempt is 1&#10;                if(attemptsMade &lt;= maxRetries) { // This means we have retries left&#10;                    LOGGER.warn(&quot;Task '{}' FAILED on attempt {}. Re-dispatching for retry... (Max retries: {})&quot;, taskName, attemptsMade, maxRetries);&#10;                    dispatchTask(dagId, taskNode);&#10;                } else { // This means attemptsMade == maxRetries + 1, which was the final attempt&#10;                    LOGGER.error(&quot;Task '{}' FAILED on final attempt {}. Initiating failure propagation.&quot;, taskName, attemptsMade -1); // Log the attempt number that failed&#10;                    redisTemplate.opsForValue().set(String.format(&quot;dag:%s:task:%s:status&quot;, dagId, taskName), &quot;FAILED&quot;);&#10;                    propagateFailure(dagId, taskName);&#10;                }&#10;            }&#10;        } catch (Exception e) {&#10;            LOGGER.error(&quot;Unexpected error during failure handling for task '{}', DAG '{}'&quot;, taskName, dagId, e);&#10;        }&#10;    }&#10;&#10;    // --- propagateFailure (Refined logic - Unchanged) ---&#10;    public void propagateFailure(String dagId, String failedTaskName) {&#10;        try {&#10;            String dagJson = redisTemplate.opsForValue().get(&quot;dag:&quot; + dagId + &quot;:definition&quot;);&#10;            if (dagJson == null) { return; }&#10;            JsonNode dagPayload = objectMapper.readTree(dagJson);&#10;            dagPayload.get(&quot;tasks&quot;).forEach(taskNode -&gt; {&#10;                if (taskNode.has(&quot;depends_on&quot;)) {&#10;                    taskNode.get(&quot;depends_on&quot;).forEach(depNode -&gt; {&#10;                        if (depNode.asText().equals(failedTaskName)) {&#10;                            String childTaskName = taskNode.get(&quot;name&quot;).asText();&#10;                            String childStatusKey = String.format(&quot;dag:%s:task:%s:status&quot;, dagId, childTaskName);&#10;                            String currentChildStatus = redisTemplate.opsForValue().get(childStatusKey);&#10;                            if (&quot;PENDING&quot;.equals(currentChildStatus)) {&#10;                                redisTemplate.opsForValue().set(childStatusKey, &quot;UPSTREAM_FAILED&quot;);&#10;                                LOGGER.warn(&quot;Propagating failure: Task '{}' marked as UPSTREAM_FAILED.&quot;, childTaskName);&#10;                                propagateFailure(dagId, childTaskName);&#10;                            }&#10;                        }&#10;                    });&#10;                }&#10;            });&#10;        } catch (IOException e) {&#10;            LOGGER.error(&quot;Failed to read DAG definition during failure propagation for DAG ID: {}&quot;, dagId, e);&#10;        }&#10;    }&#10;&#10;    // --- getDagStatus (Unchanged) ---&#10;    public DagStatusResponse getDagStatus(String dagId) {&#10;        try {&#10;            String dagJson = redisTemplate.opsForValue().get(&quot;dag:&quot; + dagId + &quot;:definition&quot;);&#10;            if (dagJson == null) { return null; }&#10;            JsonNode dagPayload = objectMapper.readTree(dagJson);&#10;            String dagName = dagPayload.get(&quot;dagName&quot;).asText();&#10;            List&lt;DagStatusResponse.TaskStatus&gt; taskStatuses = new ArrayList&lt;&gt;();&#10;&#10;            for (JsonNode taskNode : dagPayload.get(&quot;tasks&quot;)) {&#10;                String taskName = taskNode.get(&quot;name&quot;).asText();&#10;                String status = redisTemplate.opsForValue().get(String.format(&quot;dag:%s:task:%s:status&quot;, dagId, taskName));&#10;                status = (status == null) ? &quot;PENDING&quot; : status;&#10;                String logsJson = redisTemplate.opsForValue().get(String.format(&quot;dag:%s:task:%s:logs&quot;, dagId, taskName));&#10;                List&lt;String&gt; logs = (logsJson != null) ? objectMapper.readValue(logsJson, new TypeReference&lt;&gt;() {}) : Collections.emptyList();&#10;                List&lt;String&gt; dependsOn = new ArrayList&lt;&gt;();&#10;                if (taskNode.has(&quot;depends_on&quot;)) {&#10;                    for (JsonNode depNode : taskNode.get(&quot;depends_on&quot;)) { dependsOn.add(depNode.asText()); }&#10;                }&#10;                taskStatuses.add(new DagStatusResponse.TaskStatus(taskName, status, dependsOn, logs));&#10;            }&#10;            return new DagStatusResponse(dagId, dagName, taskStatuses);&#10;        } catch (IOException e) {&#10;            LOGGER.error(&quot;Failed to construct DAG status for ID: {}&quot;, dagId, e);&#10;            return null;&#10;        }&#10;    }&#10;&#10;&#10;    // --- HELPER METHODS (Unchanged) ---&#10;    private JsonNode findTaskNode(JsonNode dagPayload, String taskName) {&#10;        for (JsonNode task : dagPayload.get(&quot;tasks&quot;)) {&#10;            if (task.get(&quot;name&quot;).asText().equals(taskName)) { return task; }&#10;        }&#10;        return null;&#10;    }&#10;&#10;    private String generateK8sJobName(String dagId, String taskName) {&#10;        String cleanDagId = dagId.replaceAll(&quot;[^a-z0-9-]&quot;, &quot;&quot;).toLowerCase();&#10;        String cleanTaskName = taskName.replaceAll(&quot;[^a-z0-9-]&quot;, &quot;&quot;).toLowerCase();&#10;        String base = String.format(&quot;helios-%s-%s&quot;,&#10;                cleanDagId.substring(Math.max(0, cleanDagId.length() - 8)),&#10;                cleanTaskName);&#10;        base = base.substring(0, Math.min(base.length(), 45)).replaceAll(&quot;-$&quot;, &quot;&quot;);&#10;        return base + &quot;-&quot; + Long.toString(System.nanoTime() % 100000, 36);&#10;    }&#10;&#10;    private V1Job createK8sJobDefinition(String jobName, String dagId, JsonNode taskNode) throws IOException {&#10;        String image = taskNode.get(&quot;image&quot;).asText();&#10;        List&lt;String&gt; command = objectMapper.convertValue(taskNode.get(&quot;command&quot;), new TypeReference&lt;List&lt;String&gt;&gt;() {});&#10;        String taskName = taskNode.get(&quot;name&quot;).asText();&#10;        String cleanTaskNameK8s = taskName.replaceAll(&quot;[^A-Za-z0-9\\-_.]&quot;, &quot;&quot;).toLowerCase();&#10;        cleanTaskNameK8s = cleanTaskNameK8s.substring(0, Math.min(cleanTaskNameK8s.length(), 63)).replaceAll(&quot;^-|-$&quot;, &quot;&quot;);&#10;        String cleanDagIdK8s = dagId.replaceAll(&quot;[^A-Za-z0-9\\-_.]&quot;, &quot;&quot;).toLowerCase();&#10;        cleanDagIdK8s = cleanDagIdK8s.substring(0, Math.min(cleanDagIdK8s.length(), 63)).replaceAll(&quot;^-|-$&quot;, &quot;&quot;);&#10;&#10;        V1ObjectMeta jobMeta = new V1ObjectMeta()&#10;                .name(jobName)&#10;                .namespace(&quot;helios&quot;)&#10;                .putLabelsItem(&quot;app&quot;, &quot;helios-task&quot;)&#10;                .putLabelsItem(&quot;helios-dag-id&quot;, cleanDagIdK8s)&#10;                .putLabelsItem(&quot;helios-task-name&quot;, cleanTaskNameK8s);&#10;&#10;        V1Container container = new V1Container()&#10;                .name(cleanTaskNameK8s.substring(0, Math.min(cleanTaskNameK8s.length(), 50)) + &quot;-cont&quot;)&#10;                .image(image)&#10;                .command(command);&#10;&#10;        V1PodSpec podSpec = new V1PodSpec()&#10;                .restartPolicy(&quot;Never&quot;)&#10;                .addContainersItem(container);&#10;&#10;        V1PodTemplateSpec template = new V1PodTemplateSpec()&#10;                .metadata(new V1ObjectMeta()&#10;                        .putLabelsItem(&quot;app&quot;, &quot;helios-task-pod&quot;)&#10;                        .putLabelsItem(&quot;job-name&quot;, jobName)&#10;                        .putLabelsItem(&quot;helios-dag-id&quot;, cleanDagIdK8s)&#10;                        .putLabelsItem(&quot;helios-task-name&quot;, cleanTaskNameK8s))&#10;                .spec(podSpec);&#10;&#10;        V1JobSpec jobSpec = new V1JobSpec()&#10;                .ttlSecondsAfterFinished(3600)&#10;                .backoffLimit(0)&#10;                .template(template);&#10;&#10;        return new V1Job()&#10;                .apiVersion(&quot;batch/v1&quot;)&#10;                .kind(&quot;Job&quot;)&#10;                .metadata(jobMeta)&#10;                .spec(jobSpec);&#10;    }&#10;}&#10;" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>